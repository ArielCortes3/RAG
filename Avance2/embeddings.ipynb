{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un1AFY0tCj0P",
        "outputId": "778e2df5-5f73-4922-af09-65c544ae0d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai faiss-cpu tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import faiss\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "b4SFYWg7Ck5q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "API_KEY = userdata.get('MY_OPENAI_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd4jr2NzGYJv",
        "outputId": "ae4167c4-3358-48bf-aa98-5a5f6495c810"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key= API_KEY)"
      ],
      "metadata": {
        "id": "TRMv3GEMJGkr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/RAG Recordings/\"\n",
        "os.makedirs(save_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "wnv57fluDMSR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_tokens=400, model_name=\"text-embedding-3-small\"):\n",
        "\n",
        "    enc = tiktoken.encoding_for_model(model_name)\n",
        "\n",
        "    sessions = re.split(r'---\\s*.*?\\s*---', text)\n",
        "\n",
        "    chunks_with_meta = []\n",
        "\n",
        "    for i in range(1, len(sessions), 2):\n",
        "        session_id = sessions[i].strip()\n",
        "        session_text = sessions[i+1].strip()\n",
        "\n",
        "        sentences = re.split(r'(?<=[.!?]) +', session_text)\n",
        "\n",
        "        current_chunk, current_len = [], 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = len(enc.encode(sentence))\n",
        "            if current_len + tokens > max_tokens:\n",
        "                chunks_with_meta.append({\n",
        "                    \"session_id\": session_id,\n",
        "                    \"text\": \" \".join(current_chunk)\n",
        "                })\n",
        "                current_chunk, current_len = [sentence], tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_len += tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks_with_meta.append({\n",
        "                \"session_id\": session_id,\n",
        "                \"text\": \" \".join(current_chunk)\n",
        "            })\n",
        "\n",
        "    return chunks_with_meta"
      ],
      "metadata": {
        "id": "_lfWPzeZCo7C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_path = \"/content/drive/MyDrive/RAG Recordings/all_transcriptions.txt\"\n",
        "with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chunks = chunk_text(text)\n"
      ],
      "metadata": {
        "id": "y5vwnz0HDVLR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(chunks, model=\"text-embedding-3-small\"):\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        response = client.embeddings.create(\n",
        "            model=model,\n",
        "            input=chunk[\"text\"]\n",
        "        )\n",
        "        embeddings.append(response.data[0].embedding)\n",
        "    return np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "embeddings = get_embeddings(chunks)\n"
      ],
      "metadata": {
        "id": "qRNcPUuTCtFI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "faiss_path = os.path.join(save_path, \"recordings_index.faiss\")\n",
        "chunks_path = os.path.join(save_path, \"recordings_chunks.json\")\n",
        "\n",
        "faiss.write_index(index, faiss_path)\n",
        "\n",
        "with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "j4bA6LzYCx65"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}